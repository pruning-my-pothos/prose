---
title: "Undetected AI Exam Answers"
startDate: 2025-01-11T17:24:34.404Z
type: "smidgeon"
topics: ["Artificial Intelligence"]
citation:
  title: "A real-world test of artificial intelligence infiltration of a university examinations system: A “Turing Test” case study"
  authors: ["Peter Scarfe", "Kelly Watcham", "Alasdair Clarke", "Etienne Roesch"]
  journal: "ACM Transactions on Computer-Human Interaction"
  year: 2024
  url: "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0305354"
---

import AcademicReference from "../../components/mdx/AcademicReference.astro"
import QuoteCard from "../../components/mdx/QuoteCard.astro"

Researchers submitted entirely AI-generated exam answers to the undergraduate psychology department
of a “reputable” UK university. The vast majority went undetected and the AI answers achieved higher
scores than real students.

“We report a rigorous, blind study in which we injected 100% AI written submissions into the
examinations system in five undergraduate modules, across all years of study, for a BSc degree in
Psychology at a reputable UK university. We found that **94% of our AI submissions were
undetected**. The grades awarded to our AI submissions were on average **half a grade boundary
higher than that achieved by real students**. Across modules there was an 83.4% chance that the AI
submissions on a module would outperform a random selection of the same number of real student
submissions.”

I have to assume educators are swiftly moving to hand-written exams under supervised conditions and
oral exams. Anything else seems to negate the point of exams.
