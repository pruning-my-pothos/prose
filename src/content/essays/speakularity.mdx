---
title: "Flirting with the Speakularity"
description: "Reaching the moment when voice transcription becomes accurate, fast, private, cheap, and convenient enough to become ubiquitous"
updated: "2024-02-05"
startDate: "2024-02-05"
cover: "../../images/covers/paleo@2x.png"
type: "essay"
topics: ["Design", "Artificial Intelligence", "Language Models"]
growthStage: "budding"
toc: true
draft: true
---

import GridColumns from "../../components/mdx/GridColumns.astro";
import References from "../../components/mdx/References.astro";
import ReferencesLink from "../../components/mdx/ReferencesLink.astro";
import TweetEmbed from "../../components/mdx/TweetEmbed.astro";
import Video from "../../components/mdx/Video.astro";
import Accordion from "../../components/mdx/Accordion.astro";
import VoiceQueries from "../../components/unique/speakularity/VoiceQueries.astro";
import Subtext from "../../components/mdx/typography/Subtext.astro";

<IntroParagraph>I hit the Speakularity sometime back in August of 2023. The Speakularity is the crossover moment when you start speaking to your computer more than typing into it. It's when voice transcription becomes fast, accurate, private, cheap,<Footnote idName={1}>“Cheap” is a relative concept, but for it to become widespread it needs to be virtually free. You don't pay to use your keyboard, why would you pay to use your voice?</Footnote> and convenient enough to use all day, every day.</IntroParagraph>

Journalist [Matt Thompson](https://twitter.com/mthomps)
[coined the phrase](https://snarkmarket.com/2010/6498/) in 2010 when speech-to-text technology was
still in its infancy and automated captions were
[“cringe-inducing.”](https://snarkmarket.com/2010/6498/#:~:text=Cringe%2Dinducing%2C%20right%3F) It
was a hopeful prediction in a dire time for voice input and audio transcription.

Companies were just beginning to make voice input “happen” around 2010. While we've had specialised
dictation software like [Dragon Dictate](https://en.wikipedia.org/wiki/DragonDictate) since the
1990's, it certainly wasn't cheap,<Footnote idName={2}>The first release of Dragon Dictate in 1990
[cost $9,000](https://www.nytimes.com/1990/03/20/business/company-news-dragon-introduces-voice-typewriter.html),
but then dropped to a more
[“reasonable” $695](https://www.nytimes.com/1999/03/01/business/technology-market-place-dragon-systems-a-former-little-guy-gets-ready-for-market.html)
by 1997</Footnote> easily accessible, or integrated into existing user workflows. Google was the
first to try and change this
[in late 2008](https://www.nytimes.com/2008/11/14/technology/internet/14voice.html) when they added
voice search to their iOS app.

<GridColumns columns={2} gridGap="1rem" maxWidth="1000px" gridTemplateColumns="3fr 2fr">

<BasicImage
framed
width="600px"
src="/images/posts/speakularity/google-voice-1.jpg"
alt="Voice input integrated into Google Search, 2008"
margin="0.5rem auto" />

<BasicImage
framed
width="330px"
src="/images/posts/speakularity/google-voice-2.png"
alt="Voice input integrated into Google Search, 2008"
margin="0.5rem auto"
 />

</GridColumns>

<Subtext>Google voice input on iOS in 2008 – Sources: [Technologizer](https://technologizer.com/2008/11/17/hands-on-with-googles-voice-search-for-iphone/index.html), [PC World](https://www.pcworld.com/article/532047/search-2.html)</Subtext>

Because it was free and seamlessly integrated into a familiar task, it was far more accessible than
previous transcription technologies. You didn't have to buy, install, and learn specialised
software; you just tapped a button in your search bar. More importantly, they eased people into the
idea we might be able to _talk_ to our devices one day. Apple quickly built on this idea by
releasing Siri in
[October 2011](https://en.wikipedia.org/wiki/Siri#:~:text=its%20release%20on-,4%20October%202011,-%2C%20removing%20the%20separate)
– the first attempt at a general purpose voice assistant.

<BasicImage
src="/images/posts/speakularity/siri-phone.jpg"
alt="Siri's original interface, 2011"
width="400px"
showalt
sourceUrl="https://www.forbes.com/sites/nicoleperlroth/2011/10/12/siri-was-born-a-man-and-other-things-you-dont-know-about-apples-new-personal-assistant/"
sourceTitle="Forbes"/ >

This kicked off a decade of voice assistant hype and development. Amazon and Microsoft released
their assistants [Alexa](https://arc.net/l/quote/ygynnskd) and
[Cortana](https://arc.net/l/quote/yherkzah) in 2014. Google upgraded their voice search into the
fully featured
[Google Assistant](https://blog.google/products/assistant/io-building-next-evolution-of-google/)
in 2016. These companies also began to make harware plays by selling single-purpose devices for
their voice assistants like [Google Home](https://home.google.com/intl/en_uk/welcome/),
[Amazon Echo](https://www.amazon.co.uk/b?ie=UTF8&node=14100223031),
[Apple HomePod](https://www.apple.com/uk/homepod/). We entered a world where you could now summon
facts, weather forecasts, music, and direct the lights in your house to cycle through a rainbow with
simple voice commands.

[Row of voice assistant logos and devices]

While useful, these voice assistants only accepted short, structured queries like:

<VoiceQueries />

The brevity of these inputs was both a feature and a design limitation. They were quick to interact
with, but masked a lot of the rough edges. When you're only saying a few words, you don't notice
that the accuracy rate is less-than-perfect. They didn't need to be that _good_ at voice
recognition. They didn't have to deal with an unlimited range of specialised vocabularies, speech
acts, or contexts.

They didn't have to perform at the level of systems that have to handle long, continous voice
dictations or transcribe hours of audio. They also trained us to think about voice input as a
short-form medium. Most people still associate voice input with short, directive commands Few people
think about long-form transcription when they hear “voice input.”

If you tried to use voice for longform inputs back then, the accuracy issues would have immediately
become apparent. I experienced this firsthand in 2011<Footnote idName={3}>Coincidently, the year
after Matt Thompson's Speakularity prediction</Footnote> when I snapped my right wrist into multiple
pieces during a hungover attempt to tackle an enormous woman in a rugby game.

As a student with a few weeks until finals, this wasn't the best moment to have a metal pin
surgically inserted into my dominant writing hand. The student centre handed me a copy of Dragon
Dictate and sent me off to transcribe all my exams and essays.

<BasicImage width="1000" framed src="/images/posts/speakularity/dragon-dictate-2011.png" alt="Dragon Dictate's Mac interface circa 2014" showalt sourceUrl="https://www.itwriting.com/blog/8375-review-dragon-dictate-4-for-the-mac.html" sourceTitle="Tim Anderson's IT Writing" />

Despite extensively training it on my voice, it filled sentences with strings of misinterpretations.
I spent the rest of the semester painstakingly correcting absurd transcriptions at a snail's pace.
Resulting in final work resembling staccato poetry, riddled with grammatical errors and misplaced
words. The Speakularity felt very far away at this point.

Jump forward a dozen years and we're in a very different place. Voice is now one of my primary input
methods. A series of advancements over the last few years have made the Speakularity possible.

## Living in the Speakularity

I was surprised at how quickly and easily voice became one of the main ways I interact with my
computer. Every morning I sit at my desk and “write” my reflections, tasks, and top-of-mind concerns
by speaking out loud to my Macbook. When I open Slack to catch-up on my co-worker's messages, I dash
off responses with my voice. When I have an idea for a new essay, I create the first draft by
talking out loud. When I have to reply to a particularly tiresome and beaurocratic email, I get over
that resistance by verbalising it.

How am I doing this articulated magic?

I have a little app called [SuperWhisper](https://superwhisper.com/)<Footnote idName={4}>This is
currently the best app on the market by a good margin, but I'll discuss different options and design
considerations later on.</Footnote> that lives in my menu bar. It stays out of the way unless I
summon it by hitting a keyboard shortcut. I use `Ctrl + W`, but you can set it to whatever you like.
When called, it opens a small pop-over window and begins recording.

[ BasicImage width="600px" src="/images/posts/speakularity/superwhisper0.jpg" ]

I say what I want to say, and press the same keyboard shortcut. After a brief loading period, the
transciption text is pasted inline wherever I have my cursor placed.

[ video of superwhisper ]

In that loading period, Superwhisper feeds my audio file into an AI model called
[Whisper](https://openai.com/research/whisper). It's an
[open source](https://github.com/openai/whisper) voice-to-text model created by
[OpenAI](https://openai.com/) in 2022. SuperWhisper downloads Whisper onto my local machine and
gives me an easy way to use it. It even lets me pick which size model I want – larger models tend to
be slower, but more accurate.

<BasicImage
framed
width="800px"
src="/images/posts/speakularity/superwhisper-models.png" />

The inline paste feature is critical. It means I can transcribe my voice into any app without
worrying about compatability. The single keyboard shortcut to record and paste is also important. It
only takes me one swift motion to be in transcription mode.

I've crossed some talking tipping point now. When I'm in a room alone, I will opt to use voice input
over typing every time. Even when my partner is sitting next to me I can't help but switch to voice
occasionally.<Footnote idName={2}>I've given him permission to veto this behaviour anytime he
wants.</Footnote>

I now find myself irrationally frustrated when I'm in contexts where I can't use my voice to
transcribe, such as shared spaces with other people, or zoom calls. I begrudingly take notes by
sloooooooowly banging them out with my stubby little fingers.

Why am I so enamoured with talking to my machine? What are the benefits of using voice input?

### Speed

The big, obvious, shining benefit of voice transcription is speed. I type at a respectable 63 words
per minute.<Footnote idName={6}>You can test your own typing speed
[here](https://www.typingtest.com/).</Footnote> This is slightly better than your average
computer-user at 40 words per minute. Even the most intense typists among us – the folks with split
hand [Moonlanders](https://www.zsa.io/moonlander/) and blank keys – only get up to about 120 words
per minute.

You and I can speak at [160 words per minute](https://arc.net/l/quote/jmmhfczp) without even trying.
That's a comfortable cadence for audiobooks and relaxed conversations, but it's still a 4x increase
over typing. Frenetic auctioneers and policy debaters get closer to 250-300 words per minute.

[ quick viz of spoken words vs typing speeds ]

When you're transcribing instead of typing, you can viscerally feel yourself flying through tasks in
a quarter of the time. Documents fill with words. Emails are dashed off. Drafts feel like they knock
themselves out.

We are good talkers, and good at thinking while speaking. Consider how much easier it is to roll
over to a colleague in the same room to chat through a problem vs. compose an email to them. The
majority of us would choose voice-to-voice communication every time.

### Friction

The flying speed of voice has some interesting knock-on effects. When you know it's only going to
take you a quarter of the time to ”write” something, you're much more willing to do it. You feel
less emotional friction.

This is certainly true of tasks where the thing you're doing is a predictable, utility chore. The
kind that doesn't require deep thought. Sending a simple email. Capturing post-meeting notes for
yourself. Replying to a co-worker when you already know the answer to their question.

But it's also true of trickier cognitive tasks, like figuring out the main point of a large,
sprawling essay you're trying to wrangle into a coherent narrative. Or considering how to give
critical feedback to a colleague. Or narrowing in on a concise, clear problem statement in a feature
spec. It's easier to get a first pass done by thinking out loud.

### Emotional Processing

Perhaps unsurprisingly, journaling with your voice rather than your fingers has a very different
emotional resonance. Typing “I feel desperately sad” and _audibly saying_ “I feel desperately sad”
are not the same experience. There is something about hearing your own voice make statements that
lends itself well to acknowledging and accepting them.

Or at least it does for me. There's a deeper reality to the words. I can type anything onto a screen
and stay fairly disembodied from it. I have trouble using my voice and breath and body to say
something, hear myself saying it, and still remain disconnected.

Perhaps it's because I usually use my voice to let others hear me. Rather than so my computer can
send an audio file to an API that will feed it into a neural network and return a text string for me
to read. Describing our feelings out loud to other people makes them much harder to ignore, brush
over, or pretend we never felt them. Now someone else knows. This is the basis of much of talk
therapy.

Talking to my computer feels like a bit of loophole in this system. It's nowhere near as effective
as talking to a real person who can talk back, but it's a cheap, convenient, moderately effective
alternative for the kind of low-stakes, everyday emotional processing that you might otherwise do
with friends and family.<Footnote idName={1}>For the record, you should still talk to your friends
and family about your emotions, but rather than whine to them for hours, voice journaling with a
computer is a good way to sort through your feelings ahead of time.</Footnote>

### Rubber Ducking and Spitballing

For processing less emotional but still fuzzy and unformulated ideas, voice to text can help you
[rubber duck](https://en.wikipedia.org/wiki/Rubber_duck_debugging). “Rubber ducking” is
traditionally a debugging method for programmers. You explain the problem you're trying to solve to
a either an inanimate object – like a rubber duck – or a human who's happy to act as a rubber duck.
You tell them what you've attempted to do so far to solve it and in the process you sometimes figure
out a solution. Or at least gain insight into parts of the problem that you've overlooked and new
angles of attack.

<BasicImage src="/images/covers/rubberduck@2x.png" width="600" />

But rubber ducking isn't just for programming. You can rubber duck your way through any kind of
problem. I find it especially helpful when I'm trying to construct the core arguments of a piece of
writing, or consider the trade-offs of various design decisions.

I find that dumping out all my thoughts in front of me helps me to both see and hold the full
context of the problem in your mind at once. And then allows you to construct solutions that takes
the full picture into account.

## Historical Tipping Points

[What changed in voice transcription that made it useable?]

[At this point, a few of you are thinking “why are you trying to hard sell me on a feature I've had
in my OS for over a decade?” And that's a good question!]

It wasn't just mobile devices and stand-alone assistant pods that because voice enabled after 2010.
Apple introduced speech-to-text dication to MacOS back in 2012 as part of
[Mountain Lion](https://arc.net/l/quote/gtvxklyl).<Footnote idName={1}>Getting these historical
dates straight has been tricky, because Apple added a command-only speech recognition system called
[Voice Navigator](https://en.wikipedia.org/wiki/Voice_Navigator) in 1989, but it didn't support
dicatation. A few third-party software systems enabled dictation on Mac earlier, but you had to
purchase them separately.</Footnote>Microsoft introduced it even earlier with
[Windows XP in 2006](https://en.wikipedia.org/wiki/Windows_Speech_Recognition). But operating
systems tend to hide dication deep inside settings menus. They don't advertise them in shiny release
accouncements, and they've traditionally been thought of as assistance technologies for people with
disabilities.

Why am I suddenly raving about this? What changed in speech to text technologies that made the
experience of using them remarkably different?

This lack of fanfare is likely because, until _very_ recently, speech-to-text systems were
relatively shit. Much like young Shailesh in 2011, anyone who tried to use dictation for long-form
input would've found they were better off typing it. Most people still have this experience.

Back in February of 2024 I ran a poll on Twitter to ask how many people regularly used voice input,
and for people who never used it, what the main reason was.

<TweetEmbed tweetId="1755556368893751475" />

Well over half (53.9%) of the 892 respondents never use voice. And a full 90% of people use it less
than 20% of the time. This surprised me! My Twitter network is full of exploratory and highly
technically literate nerds. Exactly the kind of people who might experiment with voice input.

The two main reasons people don't use it are that it would annoy people around them, and that it
makes too many mistakes, closely followed by not having a good use case. I'll address noise,
environment issues, and use cases a bit later on, but lets talk about accuracy for a moment.

### The Accuracy Gap

Above all else, accuracy is the most important metric for any speech-to-text system. And it's the
metric that's shifted the most over the last few years. I think most people are stuck in a
perception gap. Their previous experience with older, less accurate voice input makes them skeptical
that newer ones are drastically better.

The academics and researchers in charge of improving speech-to-text systems measure their accuracy
in “Word Error Rates” or WER. Human transcribers have an average WER of 5%, meaning they get 5 out
of 100 words wrong.

<Accordion header="How exactly do researchers measure speech-to-text accuracy?">

I thought it would be simple to look up the accuracy rates of various speech-to-text systems over
the last few decades. Oh, was I wrong! It turned into quite a research rabbit hole.

There are a number of different benchmarks researchers use to measure the accuracy of each system.
Their performance varies based on the quality of the audio, the accents of the speakers, the
language being spoken, and the style of speech. For example, organic human-to-human conversation is
harder to transcribe than slower, structured speech like reading an article.

</Accordion>

In trying to research accuracy improvements to speech systems over the last few decades, I've landed
in the tangled problem of how we evaluate these systems. The question becomes _who_ are the systems
accurate for? And it turns out to be people with American or British accents speaking slowly and
clearly without much background noise or interference.

While researchers may claim a WER of under 5% on benchmarks like Switchboard or CallHome, these
don't accurately represent the kinds of speech people need transcribed in real life. Anyone with a
different accent or even a low quality microphone will have trouble being understood and have a much
higher WER.

WERs started around 30-40% in the 1990's<Footnote idName={8}>Cite</Footnote>, but slowly progressed
downwards. Over the last decade, companies continuously declared they had had broken the record of
human-level accuracy for transcription. It started in XXX when IBM announcced a WER of 5.9. Then in
2017 Google announced they'd achieved a WER of X, below the 5% mark taken as "human-level".

<BasicImage margin="2rem auto 3rem" src="/images/posts/speakularity/wer.svg" sourceTitle="Speech
Recognition Is Not Solved – Awni Hannun" sourceUrl="https://awni.github.io/speech-recognition/"
alt="WER improvements over time on the Switchboard benchmark. Switchboard is a dataset of 40 phone
chats between two native English speakers" showalt />

So it turns out there is no such thing as a single WER rate for a given model. Its performance
depends on the speaker, the amount of background noise, and the subject matter being talked about.

Deepgram [measured](https://arc.net/l/quote/rxyldyks) the accuracy rate of the current most popular
models across a range of tasks.<Footnote idName={9}>We should take this with some grains of salt,
because Deepgram offers a paid speech-to-text API that competes with every model they've listed
here. We'll give them the benefit of the doubt for this graph, but don't take it as
Science.</Footnote> Podcasts had the lowest WER likely because it's one or two people speaking
clearly into a good quality mic. Phone calls have a higher WER given that people on calls tend to
speak faster, more casually, and talk over one another. Plus the audio quality is rarely stellar.

[BasicImage framed src="/images/posts/speakularity/wer-rates.webp" alt="Average WER rate for popular
ASR models across a range of tasks" sourceUrl="https://arc.net/l/quote/atksqtsc"
sourceTitle="Deepgram" showalt width=1400]

These models can fail because they don't have the same kind of context about us as someone we're
speaking to. They don't know our professions, what tasks we're currently trying to do, or any of the
specialised vocabulary we might use.

In medical contexts they won't recognise words like XXX or XXX. In earnings meetings they won't know
company names like XXX and XXX. When a product manager is trying to write a product spec, they're
not going to know what XXX or XXX is.

Benchmarks from 2021 were 80-85% –
[How Accurate Is Speech-to-Text In 2024?](https://deepgram.com/learn/best-speech-to-text-apis)

Around 2012, new machine learning methods like deep learning and recurrent neural networks (RNNs)
started to improve the accuracy rate for speech recognition systems
([Beaufays, Google Resarch 2015](https://research.google/blog/the-neural-networks-behind-google-voice-transcription/)).
Previous architectures like gaussian mixture models (GMMs) and hidden markov models required systems
to be fine-tuned and optimized on specific datasets.

"Deep Speech 2, developed by Baidu in 2015, used RNNs and end-to-end learning, achieving human-level
transcription accuracy for certain languages." - need source

"Advanced models like WaveNet (by DeepMind) improved speech synthesis, complementing transcription
technologies." - need source, 2019

"Self-supervised learning (e.g., wav2vec 2.0 by Facebook AI) enabled models to learn from raw audio,
reducing the need for labeled data and supporting low-resource languages." - need source, 2020

In September of 2022, OpenAI released a model called Whisper.

Now, I say this as someone who's voice meshes well with these systems. My British-American hybrid
accent is a little bit strange, but it's certainly not drastically different from the kinds of
voices these systems are trained on, which are primarily American accents. I have no doubt these
models are much less accurate for people who speak English as a second language, or have accents
from anywhere else in the world.

[ Data about Whisper's accuracy on non-english accents and other languages ]

---

Before August, I never considered pure voice input a feasible option. Because, like all of you, my
personal experience of native transcription was, in a word, shit.

Let's compare a few sentences transcribed with Whisper versus with Apple's native TTS. I've picked
some sentences from this piece and read them out loud to each system:

#### Whisper output

My British-American hybrid accent is a little bit strange, but it's certainly not drastically
different from the kinds of voices these systems are trained on, which are primarily American
accents. These models are much less accurate for people who speak English as a second language, or
have accents from anywhere else in the world.

#### Apple native TTS output

My British American hybrid accent is a little bit strange, but it certainly not drastically
different from the kinds of voices. The systems are trained on which are primarily American accents.
These models are much less accurate for people who speak English as a second language who who have
accents from anywhere else in the wor?

<Spacer size="small" />

#### Whisper output

In Matt's original specularity prediction, they identified a few major blockers to the specularity
happening, namely speed, cost and quality. While these are all important, they forget that one of
the biggest blockers is design and ease of use.

#### Apple native TTS output

In maths, original, secularity prediction, they identified a few major blockers to the secularity
happening, namely, speed cost and quality. All these more importantly, forget that one of the
biggest blockers design and ease of use.

---

Hopefully the difference between these two outputs is clear.

The mistakes that Mac TTS system makes seem minor. But when you look at it you realise you're going
to have to rewrite the entire thing by hand. But with Whisper I find I only need to fix one or two
words or some punctuation.

I find it particularly frustrating that Mac's text-to-speech outputs don't make grammatical sense.
They're clearly not checking whether the sentence is complete and cohesive.

This is where the way Whisper is built makes a difference. Whisper is built off similar technology
to GBT 3.5 and above. It uses the already established words earlier in the sentence to predict the
most likely next word. So it's more likely to return text that makes coherent sense as a whole. It's
not just transcribing each word one at a time.

---

How is Whisper doing this? Here's a simple diagram that clearly explains the whole system:

[BasicImage width="1100px" src="/images/posts/speakularity/whisper-diagram.png"]

Oh, you're confused? Are you not familiar with Log-Mel Spectrograms? Did you get lost somewhere
around the “Sinusoidal Positional Encoding” mark?

But something meaningfully shifted over the last (18 months?). Namely, that we (pulled transformers,
neural networks, and machine learning into the picture).

(OpenAI released Whisper in X, Y released Z. Cite some research papers of people discovering how to
make STT better)

(Add diagram of neural networks and audio data?)

You are not alone! I have only the fuzziest conceptual understanding of transformer models and
self-attention.

These advancements all led up to my inflection point over the summer. (I first tried Whisper in Tana
and realised how much easier it was to capture thoughts with voice. I then found MurmurType via
Setapp and that REALLY shifted my behaviour. The UX design and lack of friction was the turning
point.)

It's always been cheap enough – we all have free built-in voice-to-text features on Mac, Windows, or
our smartphones. It's not that fast, but it's tolerable. The primary blocker up to now has been
_accuracy_. It's just not very _good_ at recognising what we're saying.

( image of cheap, fast, and accurate )

(!! I think this claim is correct but I have to double check it and I also have to figure out how
I'm going to double check it. Hopefully someone has written a piece about why Whisper is such a
different technology to previous transcription technologies and why it's so much more accurate. I'm
fairly certain it's to do with language models and attention and transformers but I have to verify
that!!

The key thing we were missing in technologies like Dragon Dictate and early Google voice
transcription was language models. Previous transcription software could only make guesses at what
words we were using but couldn't understand the larger context they sat within. Until we got
transformers who could look at the full context of all the sentences a spoken word sits within, it's
very hard to predict what the correct version of that word is. There are far too many spoken
pronunciations that sound identical that we only understand through the context they're placed
within.)

(Diagram of transformers and attention)

"Whisper is a machine learning model for speech recognition and transcription, created by OpenAI and
first released as open-source software in September 2022"

While it's true that accuracy was one of the main blockers here, I think the user experience has
also been a sticking point.

I frankly should have hit it much earlier. The technology that makes voice-to-text transcription
fast, accurate, and cheap was released was OpenAI back in X. But I was held back by the things that
always holds back breakthrough technologies - the user experience.

## Feeling the Design Friction

In Matt's original Speakularity prediction, they identified a few major blockers to the speakularity
happening: speed, cost, and quality. While these are all important, they forgot one of the biggest
blocker: **design and ease of use**.

I didn't start consistently using voice input until I had an app that allowed me to capture voice
input and paste it inline with two quick keyboard tap.

Need a suite of settings to interpret what you've said.

Running language models over the output of transcription models is the perfect combination.

Want to run language models over the transcription for:

- Common names, brands, strange words it doesn't know but you say all the time: Jungwon / Jeng Wan,
  John one. Elicit / illicit. Speakularity / Specularity.
- Telling it to write in british english and not american english
- Pulling out tasks and actions, rather than just transcribing
- Transforming speech style text into written style text: la parole and la language. Remove ums,
  pauses, don't transcribe ellipses, remove the word "and" from the start of every sentence.

"When I talk, it often sounds like this. I'm taking long gaps in between things I'm saying, and I
use the word "and" all the time. I often start sentences of the "and", and I'm trying to think of
the next thing to say. So I'll pause for long periods of time, but that doesn't mean I want a whole
string of ellipses in this transcription."

**[I've just realised I should make a new component for this piece where I can have people play
audio recordings of me talking and then show what was transcribed by something like Super
Whisper.]**

There are also some specialised tasks that don't lend themselves well to plain transcription.
Programming is a good example of this. It's hard to know how to describe certain kinds of syntax
using your voice. And you often need to move the cursor around.

Pokey Rule has built a system called Cursorless that supports a whole host of complex programming
functionality with only voice:

<Video src="https://www.youtube.com/embed/0ZZb12Qp6-0?si=VbLipvz48QufpZl-&amp;start=76" />

<Spacer />

## Noise Problems

There's one blaring problem with switching to voice input; you have to make a bunch of noise. Not
just noise, but distracting, disrupting speech that will interrupt the thoughts of anyone around
you.

Not only will you annoy them, you'll likely feel subconscious trying to talk to your machine in
earshot of them. It's not a good time to explore anything private or sensitive.

Subaudible microphone technology roundup.

<References>
	<ReferencesLink
		title="Speech Recognition Through the Decades: How We Ended Up With Siri"
		href="https://www.pcworld.com/article/477914/speech_recognition_through_the_decades_how_we_ended_up_with_siri.html"
		author="Melanie Pinola, PC World, 2011"
	/>
	<ReferencesLink
		title="A Brief History of Speech to Text and How it Actually Works"
		href="https://mythicalai.substack.com/p/a-brief-history-of-speech-to-text"
		author="Josh Dance, 2023"
	/>
	<ReferencesLink
		title="Audrey, Alexa, Hal, and More"
		href="https://computerhistory.org/blog/audrey-alexa-hal-and-more/"
		author="Dag Spicer, Computer History Museum, 2021"
	/>
	<ReferencesLink
		title="Speech Recognition Is Not Solved"
		href="https://awni.github.io/speech-recognition/"
		author="Awni Hannun, 2017"
	/>
	<ReferencesLink
		title="The Future of Speech Recognition: Where Will We Be in 2030?"
		href="https://thegradient.pub/the-future-of-speech-recognition/"
		author="Migüel Jetté, The Gradient, 2022"
	/>
</References>
